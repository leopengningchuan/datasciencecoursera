---
title: "Practical Machine Learning: Programming Assignment_Course Project"
author: "Leo Peng"
date: "3/9/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview

In this project, we are going to use the machine learning method to classify the movements of people.

"One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it." We will use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Based on the data collected , we can analyze how well they do the movements and classify their movements into certain types.


## Data Loading and Data Cleaning

We use the `read.csv` function and put the datasets into two dataframes `pml.training` and `pml.testing`. The "X" columns in both two datasets are unnecessary, so we get rid of them.

```{r}
setwd("/Users/LeoPeng/Desktop/JHU Data Science/Course Material/C8_Practical Machine Learning/Course Project")
pml.training <- read.csv("./pml-training.csv", header=T, na.strings=c("","NA"))
pml.testing <- read.csv("./pml-testing.csv", header=T, na.strings=c("","NA"))
pml.training$X <- NULL
pml.testing$X <- NULL
```

We use the `str()` function and `dim` to get a general idea about these two datasets.

```{r}
str(pml.training)
dim(pml.training)
str(pml.testing)
dim(pml.testing)
```

To do the further research, we have to do the data cleaning steps are following:

```{r}
# Get rid of the columns which are most 'NA'
pml.training <- pml.training[ , colSums(is.na(pml.training)) == 0]
pml.testing <- pml.testing[ , colSums(is.na(pml.testing)) == 0]

# Get rid of the first four columns which are unnecessary
pml.training <- pml.training[, -c(1:4)]
pml.testing <- pml.testing[, -c(1:4)]
```

The final datasets `pml.training` and `pml.testing` have 55 variables.


## Model Building

The next step is building the model. First we establish the library `caret`. We use function `createDataPartition()` to slice the `pml.training` to two datasets `training` and `testing`. We will use these two models to train and test the final model.

```{r}
library(caret)
inTrain <- createDataPartition(pml.training$classe, p=0.7, list=F)
training <- pml.training[inTrain, ]; testing <- pml.training[-inTrain, ]
```

Now is the most important step. We use the `train()` function to build the model. I choose the random forest method, number=3 and trControl=trainControl(method='cv').

```{r}
mod_rf <- train(classe ~., method='rf', data=training, number=3, 
                trControl = trainControl(method='cv'))
```

After 30 minutes of waiting, we get the final model. To see how well the model is, we can use this model to predict the `training` dataset and see the classes by the `table` function. 

```{r}
pred_training <- predict(mod_rf, training)
table(pred_training, training$classe)
```

The result is fantastic! The model predicts all the classes (13737 in total) in `training` dataset correctly.

Then, we can use the same method to predict the classes in `testing` datasets.

```{r}
pred_testing <- predict(mod_rf, testing)
table(pred_testing, testing$classe)
```

From the above table, we can see the model only predicts 19 classes of 5885 in `testing` dataset wrongly. The correct rate is 99.7%.


## Final Result Prediction

Finally, we will use this model to predict the `testing` dataset. We do not have the correct classes of this dataset and if our model is good enough, we can get the right prediction in a few seconds.

```{r}
predict(mod_rf, pml.testing)
```

Above is the final prediction. We submit it to the online quiz page.